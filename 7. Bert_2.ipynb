{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /opt/conda/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_hub) (1.19.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_hub) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow_hub) (3.11.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow_hub) (49.2.0.post20200712)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.2.0\n",
      "Hub version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from tensorflow.keras.models import  Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN=70\n",
    "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    " \n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer= bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_input(sentence,MAX_LEN):\n",
    "  \n",
    "  stokens = tokenizer.tokenize(sentence)\n",
    "  \n",
    "  stokens = stokens[:MAX_LEN]\n",
    "  \n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    " \n",
    "  return ids,masks,segments\n",
    " \n",
    "def create_input_array(sentences):\n",
    " \n",
    "  input_ids, input_masks, input_segments = [], [], []\n",
    " \n",
    "  for sentence in tqdm(sentences,position=0, leave=True):\n",
    "  \n",
    "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
    " \n",
    "    input_ids.append(ids)\n",
    "    input_masks.append(masks)\n",
    "    input_segments.append(segments)\n",
    " \n",
    "  return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "#x = tf.keras.layers.Dropout(0.2)(x)\n",
    "#out = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
    " \n",
    "#model = tf.keras.models.Model(\n",
    "#      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    " \n",
    "#model.compile(loss='binary_crossentropy',\n",
    "#                  optimizer='adam',\n",
    "#                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>jew mad? get fuhrerious!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>brother... a day without a blast is a day wasted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              text\n",
       "10      1                          jew mad? get fuhrerious!\n",
       "12      1  brother... a day without a blast is a day wasted"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_json('/home/jovyan/data/train.jsonl', lines=True)\n",
    "df_dev = pd.read_json('/home/jovyan/data/dev.jsonl', lines=True)\n",
    "df_test_f = pd.read_json('/home/jovyan/data/test.jsonl', lines=True)\n",
    "df = df[[\"label\",\"text\"]]\n",
    "df_dev = df_dev[[\"label\",\"text\"]]\n",
    "#df = pd.concat([df, df_dev], ignore_index=True)\n",
    "df[df.label == 1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df[\"text\"]\n",
    "y_train = df[\"label\"]\n",
    "\n",
    "x_test = df_dev[\"text\"]\n",
    "y_test = df_dev[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 500\n"
     ]
    }
   ],
   "source": [
    "df.count()\n",
    "print(x_train.count(), x_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8500/8500 [00:02<00:00, 3886.91it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs=create_input_array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 70, 50)            1526100   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 70, 64)            21248     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,549,461\n",
      "Trainable params: 1,549,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding,Dense,GlobalMaxPool1D,Dropout,Flatten,Bidirectional,LSTM\n",
    "from keras.models import Sequential\n",
    "embedding_dim=50\n",
    "model=Sequential([Embedding(input_dim=len(tokenizer.vocab),output_dim=embedding_dim,input_length=MAX_SEQ_LEN),\n",
    "                 Bidirectional(LSTM(32,return_sequences=True)),\n",
    "                 GlobalMaxPool1D(),\n",
    "                 Dense(32,activation='relu'),\n",
    "                 Dropout(0.5),\n",
    "                 Dense(1,activation='sigmoid')\n",
    "                 ])\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 425/1000 [===========>..................] - ETA: 33s - loss: 0.2170 - accuracy: 0.8690WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 425/1000 [===========>..................] - 27s 63ms/step - loss: 0.2170 - accuracy: 0.8690 - val_loss: 2.0609 - val_accuracy: 0.6005\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(inputs[0], y_train, epochs=1, batch_size=15, validation_split=.25, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.21700319647789001],\n",
       " 'accuracy': [0.869019627571106],\n",
       " 'val_loss': [2.060885190963745],\n",
       " 'val_accuracy': [0.6004706025123596]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f99b8058460>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "model.save(\"bert2_h5_model.h5\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "# model = keras.models.load_model(\"bert_h5_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 4077.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_inputs=create_input_array(x_test)\n",
    "y_pred = model.predict(test_inputs[0]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.502488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3006.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# final test data\n",
    "xtest_f = df_test_f[\"text\"]\n",
    "xtestf= create_input_array(xtest_f)\n",
    "\n",
    "y_test_f_pred = model.predict(xtestf[0]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-85-73dce8b73542>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_to_submit[\"proba\"] = y_test_f_pred\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "df_to_submit = df_test_f[[\"id\"]]\n",
    "df_to_submit[\"proba\"] = y_test_f_pred\n",
    "df_to_submit[\"label\"] = df_to_submit['proba'].map(lambda x: 1 if x > 0.5 else 0)\n",
    "df_to_submit.head()\n",
    "\n",
    "#print(df_test_f[df_test_f.label == 1])\n",
    "df_to_submit.to_csv('/home/jovyan/data/csv_to_submit' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") +'.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
